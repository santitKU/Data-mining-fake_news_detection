# -*- coding: utf-8 -*-
"""FAKE_DETECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YS58y1BsfujBci1QzXB0XTDtgWVgM1r4
"""
import pickle
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

"""DATA PREPROCESSING"""

# Load fake news data from CSV
fake_data = pd.read_csv(r"C:\Users\KU_AI_6\Desktop\fakenews\archive (1)\Fake.csv")

# Load true news data from CSV
true_data = pd.read_csv(r"C:\Users\KU_AI_6\Desktop\fakenews\archive (1)\True.csv")

# Label fake news as 1 and true news as 0
fake_data['label'] = 1
true_data['label'] = 0

# Concatenate fake and true news data
df = pd.concat([fake_data, true_data], ignore_index=True)

df.head(10)

"""EDA(EXPLORATORY DATA ANALYSIS)"""

df['title'].replace('', np.nan, inplace=True)
df['text'].replace('', np.nan, inplace=True)
df.head()

df.describe()

df.info()

#Check ratio of fake to real

y = df.label
print(f'Ratio of real and fake news:')
y.value_counts(normalize=True).rename({1: 'fake', 0: 'real'})

df.isnull().sum()

df = df.fillna('')

df["title_text"] = df["title"] + df["text"]
df["body_len"] = df["title_text"].apply(lambda x: len(x) - x.count(" "))
df.head()

#Visualize fake and real news
import matplotlib.pyplot as plt

bins = np.linspace(0, 250, 40)

plt.hist(df[df["label"]== 1]["body_len"], bins, alpha=0.5, label="Fake", color="#FF5733")
plt.hist(df[df["label"]== 0]["body_len"], bins, alpha=0.5, label="Real", color="#33FFB8")
plt.legend(loc="upper left")
plt.show()

import seaborn as sns

class_names = ['fake', 'real']
label_count = df.label.value_counts()
sns.barplot(x=label_count.index, y=label_count)
plt.title('Distribution of Fake/Real News',fontsize =14)

# Preprocessing
X = df['text'].astype(str)
y = df['label']

# Tokenization
max_features = 10000
maxlen = 100
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(X.values)
X = tokenizer.texts_to_sequences(X.values)
X = pad_sequences(X, maxlen=maxlen)

# Save the tokenizer to a file
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

#Visualizing word cloud
from wordcloud import WordCloud

titles = ' '.join(title for title in df['title'])
wordcloud = WordCloud(
    background_color='white',
    max_words=300,
    width=800,
    height=400,
).generate(titles)

plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""TRAIN TEST SPLIT"""

# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""MODELING"""

# Model
model = Sequential()
model.add(Embedding(max_features, 128, input_length=maxlen))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(196, dropout=0.4, recurrent_dropout=0.4))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Training
batch_size = 32
epochs = 5
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)

"""EVALUATION"""

loss, accuracy = model.evaluate(X_test, y_test, verbose=2)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""SAVE MODEL TO A H5 FILE FOR DEPLOYMENT"""

# Save the model to an HDF5 file
model.save('your_model.h5')


